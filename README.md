HUMANMOTION++: Hierarchical Disentangled Framework for Photorealistic Full-Body Audio-Driven Animation
https://img.shields.io/badge/Paper-ICASSP-2025-blue
https://img.shields.io/badge/Demo-YouTube-red
https://img.shields.io/badge/Dataset-PATS-orange
https://img.shields.io/github/stars/xlf0617/HumanMotionPlusPlus?style=social

Official PyTorch implementation of HUMANMOTION++, a novel framework for generating high-fidelity, temporally stable full-body animations from a single reference image and audio input. Our method achieves state-of-the-art performance in video coherence, hand gesture naturalness, and identity preservation.

üé• Generated Results
<div align="center"> <table> <tr> <td align="center"><img src="./assets/result1.gif" width="200"></td> <td align="center"><img src="./assets/result2.gif" width="200"></td> <td align="center"><img src="./assets/result3.gif" width="200"></td> </tr> <tr> <td align="center"><img src="./assets/result4.gif" width="200"></td> <td align="center"><img src="./assets/result5.gif" width="200"></td> </tr> </table> <p> <em>Diverse examples of full-body audio-driven animations generated by HumanMotion++ from different reference images and audio inputs.</em> </p> </div>
‚ú® Features
Hierarchical Motion Decomposition: Maps speech into coarse semantic gestures and refines them with prosodic cues for natural and expressive motion.

Disentangled Synthesis Network: Dual-branch architecture that separates motion and appearance, enabling robust identity and background preservation.

Unified Face-Body Harmonization: Seamlessly blends high-fidelity facial animation with the body motion, eliminating neck seams.

Progressive Quality Enhancement: Dedicated pipelines (HandRefiner, Facial Stabilizer) to refine challenging areas like hands and fast-moving faces.

State-of-the-Art Performance: Superior results in temporal coherence (FVD), hand accuracy (HKC), and identity similarity (CSIM).

üìñ Paper Abstract
Photorealistic, audio-driven digital humans remain largely limited to talking heads and struggle with coherent full-body motion. We present HumanMotion++, a framework that generates high-fidelity, temporally stable full-body animation from a single reference image and audio. A Hierarchical Motion Decomposition first maps speech into coarse semantic gestures and then refines them with prosodic cues. A disentangled framework synthesizes motion via independent motion and appearance branches fused by cross-attention, preserving identity and background accurately, while coordinated image-audio modeling produces accurate facial and lip movements.

Read the full paper | View on arXiv

üõ†Ô∏è Model Architecture
https://./assets/framework.png
*Figure 1: Overview of the proposed HumanMotion++ framework.*

The framework consists of four main modules:

Hierarchical Motion Decomposition (HMD)

Semantic Motion Prior (SMP): A Transformer-based module that interprets speech content to generate large-scale, semantically relevant body language.

Prosodic Motion Refinement (PMR): Leverages prosodic features (pitch, energy) to add rhythmic, synchronized micro-gestures and head movements.

Disentangled Synthesis Network (DSN)

Motion Branch: Encodes skeletal movement patterns.

Appearance Branch: Extracts and preserves static appearance details (identity, clothing, background).

Cross-Attention Fusion: Selectively merges motion and appearance features in a disentangled manner.

Unified Face-Body Harmonization (UFBH)

Spatial-Temporal Consistency Loss: Ensures head-neck-torso motion coherence.

Boundary Smoothing Network: A lightweight CNN that performs imperceptible blending in the neck region.

Progressive Quality Enhancement (PQE)

HandRefiner: A specialist network that reconstructs anatomically plausible hand geometry.

Facial Stabilizer (BFVR-STC): Prevents facial feature drift during rapid head movements.

üöÄ Installation
bash
# 1. Clone this repository
git clone https://github.com/xlf0617/HumanMotionPlusPlus.git
cd HumanMotionPlusPlus

# 2. Install dependencies
pip install -r requirements.txt
üß† Usage
Inference
python
from humanmotionpp import HumanMotionPP

model = HumanMotionPP.from_pretrained("humanmotionpp-base")
result = model.generate(
    image_path="path/to/reference/image.jpg",
    audio_path="path/to/audio.wav"
)
result.save("output_video.mp4")
Training
bash
python train.py --config configs/base.yaml --data_dir /path/to/dataset
üìä Results
Method	FID ‚Üì	FVD ‚Üì	HKC ‚Üë	CSIM ‚Üë
EchoMimicV2	33.42	277.71	0.425	0.519
Sonic	24.11	211.71	0.271	0.517
Ours	26.37	207.65	0.431	0.614
üìù Citation
bibtex
@inproceedings{xie2025humanmotion++,
  title={HUMANMOTION++: A Hierarchical Disentangled Framework for Photorealistic Full-Body Audio-Driven Animation},
  author={Xie, Longfei and Fan, Qinyuan and Wang, Suli and Wang, Junqiao and Zhang, Heng and Ma, Xinyu and Yu, Mingrui and Wang, Yuze and Shi, Tianyu},
  booktitle={ICASSP 2025},
  year={2025}
}
üìÑ License
MIT License. See LICENSE for details.
ü§ù Acknowledgements
We thank the authors of [BEAT](https://github.com/PantoMatrix/BEAT) and [MimicMotion](https://github.com/MimicMotion) for their open-source contributions.